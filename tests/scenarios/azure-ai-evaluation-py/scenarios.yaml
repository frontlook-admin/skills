# Test scenarios for azure-ai-evaluation-py skill evaluation
# Each scenario tests a specific usage pattern against acceptance criteria

config:
  model: gpt-4
  max_tokens: 2000
  temperature: 0.3

scenarios:
  - name: model_config_and_quality_evaluator
    prompt: |
      Create a groundedness evaluation example using the Azure AI Evaluation SDK.
      Include model configuration with Azure OpenAI settings and run a groundedness evaluation.
    expected_patterns:
      - "from azure\\.ai\\.evaluation import GroundednessEvaluator"
      - "azure_endpoint"
      - "azure_deployment"
      - "GroundednessEvaluator\\("
    forbidden_patterns:
      - "deployment_name"
    tags:
      - quality
      - basic
    mock_response: |
      import os
      from azure.ai.evaluation import GroundednessEvaluator

      model_config = {
          "azure_endpoint": os.environ["AZURE_OPENAI_ENDPOINT"],
          "api_key": os.environ["AZURE_OPENAI_API_KEY"],
          "azure_deployment": os.environ["AZURE_OPENAI_DEPLOYMENT"],
      }

      groundedness = GroundednessEvaluator(model_config)
      result = groundedness(
          query="What is Azure AI?",
          context="Azure AI is Microsoft's AI platform.",
          response="Azure AI provides AI services and tools."
      )
      print(result["groundedness"])

  - name: nlp_evaluators_metrics
    prompt: |
      Show how to use NLP-based evaluators (F1, ROUGE, BLEU) with ground truth data.
    expected_patterns:
      - "F1ScoreEvaluator"
      - "RougeScoreEvaluator"
      - "BleuScoreEvaluator"
      - "ground_truth"
    forbidden_patterns:
      - "model_config"
    tags:
      - nlp
      - quality
    mock_response: |
      from azure.ai.evaluation import F1ScoreEvaluator, RougeScoreEvaluator, BleuScoreEvaluator

      f1 = F1ScoreEvaluator()
      rouge = RougeScoreEvaluator(rouge_type="rouge1")
      bleu = BleuScoreEvaluator()

      f1_result = f1(response="Tokyo is the capital of Japan.", ground_truth="Tokyo is Japan's capital.")
      rouge_result = rouge(response="Azure provides cloud services.", ground_truth="Azure is a cloud platform.")
      bleu_result = bleu(response="The cat sat on the mat.", ground_truth="A cat is sitting on the mat.")
      print(f1_result, rouge_result, bleu_result)

  - name: safety_evaluators_project_scope
    prompt: |
      Create a safety evaluation example using a project scope configuration.
      Use the ViolenceEvaluator and run a sample evaluation.
    expected_patterns:
      - "ViolenceEvaluator"
      - "azure_ai_project"
      - "resource_group_name"
    forbidden_patterns:
      - "model_config"
    tags:
      - safety
    mock_response: |
      import os
      from azure.ai.evaluation import ViolenceEvaluator

      azure_ai_project = {
          "subscription_id": os.environ["AZURE_SUBSCRIPTION_ID"],
          "resource_group_name": os.environ["AZURE_RESOURCE_GROUP"],
          "project_name": os.environ["AZURE_AI_PROJECT_NAME"],
      }

      violence = ViolenceEvaluator(azure_ai_project=azure_ai_project)
      result = violence(query="Tell me a story", response="Once upon a time in a peaceful village.")
      print(result["violence_score"])

  - name: custom_evaluator_decorator
    prompt: |
      Create a custom evaluator using the @evaluator decorator and run it in batch evaluation.
    expected_patterns:
      - "@evaluator"
      - "word_count"
      - "evaluate\\("
    forbidden_patterns:
      - "return 0.5"
    tags:
      - custom
      - batch
    mock_response: |
      from azure.ai.evaluation import evaluator, evaluate

      @evaluator
      def word_count_evaluator(response: str) -> dict:
          return {"word_count": len(response.split())}

      result = evaluate(
          data="data.jsonl",
          evaluators={"word_count": word_count_evaluator},
          evaluator_config={
              "default": {
                  "column_mapping": {
                      "response": "${data.response}"
                  }
              }
          }
      )
      print(result["metrics"])

  - name: batch_evaluate_with_mapping
    prompt: |
      Run batch evaluation with multiple evaluators and proper column mapping.
    expected_patterns:
      - "evaluate\\("
      - "evaluator_config"
      - "column_mapping"
      - "\\$\\{data\\.query\\}"
    forbidden_patterns:
      - "evaluators=["
    tags:
      - batch
    mock_response: |
      import os
      from azure.ai.evaluation import evaluate, GroundednessEvaluator, RelevanceEvaluator

      model_config = {
          "azure_endpoint": os.environ["AZURE_OPENAI_ENDPOINT"],
          "api_key": os.environ["AZURE_OPENAI_API_KEY"],
          "azure_deployment": os.environ["AZURE_OPENAI_DEPLOYMENT"],
      }

      groundedness = GroundednessEvaluator(model_config)
      relevance = RelevanceEvaluator(model_config)

      result = evaluate(
          data="data.jsonl",
          evaluators={
              "groundedness": groundedness,
              "relevance": relevance,
          },
          evaluator_config={
              "default": {
                  "column_mapping": {
                      "query": "${data.query}",
                      "context": "${data.context}",
                      "response": "${data.response}",
                  }
              }
          }
      )
      print(result["metrics"])

  - name: composite_qa_evaluator
    prompt: |
      Use the QAEvaluator composite evaluator with model configuration and ground truth.
    expected_patterns:
      - "QAEvaluator"
      - "ground_truth"
    forbidden_patterns:
      - "ContentSafetyEvaluator"
    tags:
      - composite
      - quality
    mock_response: |
      import os
      from azure.ai.evaluation import QAEvaluator

      model_config = {
          "azure_endpoint": os.environ["AZURE_OPENAI_ENDPOINT"],
          "api_key": os.environ["AZURE_OPENAI_API_KEY"],
          "azure_deployment": os.environ["AZURE_OPENAI_DEPLOYMENT"],
      }

      qa = QAEvaluator(model_config)
      result = qa(
          query="What is Azure?",
          context="Azure is Microsoft's cloud platform.",
          response="Azure is a cloud computing platform by Microsoft.",
          ground_truth="Azure is Microsoft's cloud computing platform."
      )
      print(result["groundedness"])

  - name: model_config_class_default_credential
    prompt: |
      Create a model configuration using AzureOpenAIModelConfiguration with DefaultAzureCredential.
    expected_patterns:
      - "AzureOpenAIModelConfiguration"
      - "DefaultAzureCredential"
      - "azure_deployment"
    forbidden_patterns:
      - "api_key"
    tags:
      - setup
      - authentication
    mock_response: |
      import os
      from azure.ai.evaluation import AzureOpenAIModelConfiguration, GroundednessEvaluator
      from azure.identity import DefaultAzureCredential

      model_config = AzureOpenAIModelConfiguration(
          azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
          credential=DefaultAzureCredential(),
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT"],
          api_version="2024-06-01",
      )

      groundedness = GroundednessEvaluator(model_config)
      result = groundedness(
          query="What is Azure AI?",
          context="Azure AI provides services for vision, speech, and language.",
          response="Azure AI offers vision and speech services."
      )
      print(result["groundedness"])

  - name: evaluate_target_callable
    prompt: |
      Evaluate a target callable using evaluate() with outputs column mapping.
    expected_patterns:
      - "target="
      - "\\$\\{outputs\\.context\\}"
      - "\\$\\{outputs\\.response\\}"
    forbidden_patterns:
      - "outputs\\.response"
    tags:
      - target
      - batch
    mock_response: |
      from azure.ai.evaluation import evaluate, GroundednessEvaluator

      def chat_app(query: str) -> dict:
          return {"response": f"Answer: {query}", "context": "source text"}

      groundedness = GroundednessEvaluator(
          {
              "azure_endpoint": "https://example.openai.azure.com",
              "api_key": "fake-key",
              "azure_deployment": "gpt-4o-mini",
          }
      )

      result = evaluate(
          data="queries.jsonl",
          target=chat_app,
          evaluators={"groundedness": groundedness},
          evaluator_config={
              "default": {
                  "column_mapping": {
                      "query": "${data.query}",
                      "context": "${outputs.context}",
                      "response": "${outputs.response}",
                  }
              }
          }
      )
      print(result["metrics"])
